{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIqC4D6KBC7R"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYplUiDxBHJg"
      },
      "source": [
        "Mount Google Drive:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDM8G1D0o5wV",
        "outputId": "8b1f5727-2e92-40b7-8bc0-8df7eab04594"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "gdrive = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpp2AswSBMSw"
      },
      "source": [
        "Copy dataset if needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_lgOt55_gy7"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/drive/MyDrive/BIAI/data' '/content/data'\n",
        "local_dataset = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8cplOcIBehG"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daz-xtTVBRkf"
      },
      "source": [
        "Prepare the Colab environment for imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZTzwza5HqD6a"
      },
      "outputs": [],
      "source": [
        "FOLDER_HOME = '/content/drive/MyDrive/BIAI' if 'gdrive' in locals() and gdrive else '.'\n",
        "\n",
        "import sys\n",
        "sys.path.append(FOLDER_HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ntiO4QaBxyq"
      },
      "source": [
        "Import the modules and define constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8D-t0d-Kqd1A"
      },
      "outputs": [],
      "source": [
        "from data import Loader, TestLogger, TrainLogger\n",
        "from network import Network, Trainer, Tester, DEVICE\n",
        "\n",
        "DATA_HOME = FOLDER_HOME + '/data'\n",
        "if 'local_dataset' in locals() and local_dataset:\n",
        "  DATA_HOME = '/content/data'\n",
        "\n",
        "IMG_DIM = 320\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 25\n",
        "\n",
        "MODEL_OUTPUT_PATH = FOLDER_HOME + '/model.pt'\n",
        "MODEL_INPUT_PATH = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5RLT-SCD1H"
      },
      "source": [
        "Define functions for training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7GQRhN4LwSXA"
      },
      "outputs": [],
      "source": [
        "def train(net, trainer, train_data, train_logger, epoch):\n",
        "  print(f'* Training epoch ({DEVICE}): {epoch}/{EPOCHS}...')\n",
        "  \n",
        "  net.train()\n",
        "  train_logger.start(epoch)\n",
        "  loss = trainer.train(net, train_data)\n",
        "  train_logger.log_epoch(loss)\n",
        "  \n",
        "  print(f'* Finished training epoch: {epoch}/{EPOCHS}')\n",
        "\n",
        "def test(net, tester, test_data, test_logger):\n",
        "  print(f'* Testing...')\n",
        "  \n",
        "  net.eval()\n",
        "  test_logger.start()\n",
        "  correct, total = tester.test(net, test_data)\n",
        "  test_logger.log_test_result(correct, total)\n",
        "  \n",
        "  print(f'Correct: {correct}/{total}, ({100 * correct / total:.2f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyU35ICyCNhH"
      },
      "source": [
        "Create all the components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ETtXYn2bsLOZ"
      },
      "outputs": [],
      "source": [
        "loader = Loader(DATA_HOME)\n",
        "train_folder, train_data = loader.load_train()\n",
        "test_folder, test_data = loader.load_test()\n",
        "classes = train_folder.classes\n",
        "\n",
        "net = None\n",
        "if MODEL_INPUT_PATH:\n",
        "  net = Network.load(MODEL_INPUT_PATH)\n",
        "else:\n",
        "  net = Network(IMG_DIM, len(classes), classes)\n",
        "\n",
        "trainer = Trainer(LEARNING_RATE)\n",
        "trainer.verbose = True\n",
        "trainer.report_freq = 4\n",
        "tester = Tester()\n",
        "\n",
        "train_logger = TrainLogger(FOLDER_HOME + '/train.csv')\n",
        "test_logger = TestLogger(FOLDER_HOME + '/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnX0gXTUCHuB"
      },
      "source": [
        "# Main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "uFf92Ugp00g5",
        "outputId": "2650ca9a-0c74-47d0-fdb2-92ce0f49b61b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Training epoch (cuda): 1/25...\n",
            "Progress: 1.11% (avg. loss: 3.499)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\PIL\\Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 25.56% (avg. loss: 3.499)\n",
            "Progress: 50.00% (avg. loss: 3.495)\n",
            "Progress: 74.44% (avg. loss: 3.493)\n",
            "Progress: 98.89% (avg. loss: 3.490)\n",
            "* Finished training epoch: 1/25\n",
            "* Testing...\n",
            "Correct: 11/329, (3.34%)\n",
            "* Training epoch (cuda): 2/25...\n",
            "Progress: 1.11% (avg. loss: 3.488)\n",
            "Progress: 25.56% (avg. loss: 3.344)\n",
            "Progress: 50.00% (avg. loss: 3.482)\n",
            "Progress: 74.44% (avg. loss: 3.314)\n",
            "Progress: 98.89% (avg. loss: 2.958)\n",
            "* Finished training epoch: 2/25\n",
            "* Testing...\n",
            "Correct: 37/329, (11.25%)\n",
            "* Training epoch (cuda): 3/25...\n",
            "Progress: 1.11% (avg. loss: 3.500)\n",
            "Progress: 25.56% (avg. loss: 2.807)\n",
            "Progress: 50.00% (avg. loss: 3.045)\n",
            "Progress: 74.44% (avg. loss: 2.722)\n",
            "Progress: 98.89% (avg. loss: 2.524)\n",
            "* Finished training epoch: 3/25\n",
            "* Testing...\n",
            "Correct: 44/329, (13.37%)\n",
            "* Training epoch (cuda): 4/25...\n",
            "Progress: 1.11% (avg. loss: 2.975)\n",
            "Progress: 25.56% (avg. loss: 3.052)\n",
            "Progress: 50.00% (avg. loss: 2.912)\n",
            "Progress: 74.44% (avg. loss: 2.818)\n",
            "Progress: 98.89% (avg. loss: 2.787)\n",
            "* Finished training epoch: 4/25\n",
            "* Testing...\n",
            "Correct: 49/329, (14.89%)\n",
            "* Training epoch (cuda): 5/25...\n",
            "Progress: 1.11% (avg. loss: 2.651)\n",
            "Progress: 25.56% (avg. loss: 2.645)\n",
            "Progress: 50.00% (avg. loss: 2.705)\n",
            "Progress: 74.44% (avg. loss: 2.782)\n",
            "Progress: 98.89% (avg. loss: 2.930)\n",
            "* Finished training epoch: 5/25\n",
            "* Testing...\n",
            "Correct: 52/329, (15.81%)\n",
            "* Training epoch (cuda): 6/25...\n",
            "Progress: 1.11% (avg. loss: 2.792)\n",
            "Progress: 25.56% (avg. loss: 2.516)\n",
            "Progress: 50.00% (avg. loss: 2.522)\n",
            "Progress: 74.44% (avg. loss: 3.191)\n",
            "Progress: 98.89% (avg. loss: 2.479)\n",
            "* Finished training epoch: 6/25\n",
            "* Testing...\n",
            "Correct: 67/329, (20.36%)\n",
            "* Training epoch (cuda): 7/25...\n",
            "Progress: 1.11% (avg. loss: 2.722)\n",
            "Progress: 25.56% (avg. loss: 2.776)\n",
            "Progress: 50.00% (avg. loss: 2.628)\n",
            "Progress: 74.44% (avg. loss: 2.519)\n",
            "Progress: 98.89% (avg. loss: 2.775)\n",
            "* Finished training epoch: 7/25\n",
            "* Testing...\n",
            "Correct: 56/329, (17.02%)\n",
            "* Training epoch (cuda): 8/25...\n",
            "Progress: 1.11% (avg. loss: 2.848)\n",
            "Progress: 25.56% (avg. loss: 2.411)\n",
            "Progress: 50.00% (avg. loss: 2.359)\n",
            "Progress: 74.44% (avg. loss: 2.783)\n",
            "Progress: 98.89% (avg. loss: 2.484)\n",
            "* Finished training epoch: 8/25\n",
            "* Testing...\n",
            "Correct: 75/329, (22.80%)\n",
            "* Training epoch (cuda): 9/25...\n",
            "Progress: 1.11% (avg. loss: 2.275)\n",
            "Progress: 25.56% (avg. loss: 2.764)\n",
            "Progress: 50.00% (avg. loss: 2.555)\n",
            "Progress: 74.44% (avg. loss: 2.440)\n",
            "Progress: 98.89% (avg. loss: 2.905)\n",
            "* Finished training epoch: 9/25\n",
            "* Testing...\n",
            "Correct: 63/329, (19.15%)\n",
            "* Training epoch (cuda): 10/25...\n",
            "Progress: 1.11% (avg. loss: 2.633)\n",
            "Progress: 25.56% (avg. loss: 2.698)\n",
            "Progress: 50.00% (avg. loss: 2.576)\n",
            "Progress: 74.44% (avg. loss: 2.553)\n",
            "Progress: 98.89% (avg. loss: 2.535)\n",
            "* Finished training epoch: 10/25\n",
            "* Testing...\n",
            "Correct: 67/329, (20.36%)\n",
            "* Training epoch (cuda): 11/25...\n",
            "Progress: 1.11% (avg. loss: 2.511)\n",
            "Progress: 25.56% (avg. loss: 2.563)\n",
            "Progress: 50.00% (avg. loss: 2.602)\n",
            "Progress: 74.44% (avg. loss: 2.641)\n",
            "Progress: 98.89% (avg. loss: 2.494)\n",
            "* Finished training epoch: 11/25\n",
            "* Testing...\n",
            "Correct: 61/329, (18.54%)\n",
            "* Training epoch (cuda): 12/25...\n",
            "Progress: 1.11% (avg. loss: 2.564)\n",
            "Progress: 25.56% (avg. loss: 2.592)\n",
            "Progress: 50.00% (avg. loss: 2.328)\n",
            "Progress: 74.44% (avg. loss: 2.322)\n",
            "Progress: 98.89% (avg. loss: 2.594)\n",
            "* Finished training epoch: 12/25\n",
            "* Testing...\n",
            "Correct: 46/329, (13.98%)\n",
            "* Training epoch (cuda): 13/25...\n",
            "Progress: 1.11% (avg. loss: 2.632)\n",
            "Progress: 25.56% (avg. loss: 2.546)\n",
            "Progress: 50.00% (avg. loss: 2.634)\n",
            "Progress: 74.44% (avg. loss: 2.709)\n",
            "Progress: 98.89% (avg. loss: 2.695)\n",
            "* Finished training epoch: 13/25\n",
            "* Testing...\n",
            "Correct: 73/329, (22.19%)\n",
            "* Training epoch (cuda): 14/25...\n",
            "Progress: 1.11% (avg. loss: 2.550)\n",
            "Progress: 25.56% (avg. loss: 2.176)\n",
            "Progress: 50.00% (avg. loss: 2.488)\n",
            "Progress: 74.44% (avg. loss: 2.513)\n",
            "Progress: 98.89% (avg. loss: 2.670)\n",
            "* Finished training epoch: 14/25\n",
            "* Testing...\n",
            "Correct: 71/329, (21.58%)\n",
            "* Training epoch (cuda): 15/25...\n",
            "Progress: 1.11% (avg. loss: 2.313)\n",
            "Progress: 25.56% (avg. loss: 2.543)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\BIAI.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000015?line=5'>6</a>\u001b[0m test_logger\u001b[39m.\u001b[39mappend \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000015?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000015?line=7'>8</a>\u001b[0m   train(net, trainer, train_data, train_logger, epoch \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000015?line=8'>9</a>\u001b[0m   net\u001b[39m.\u001b[39msave(MODEL_OUTPUT_PATH)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000015?line=9'>10</a>\u001b[0m   test(net, tester, test_data, test_logger)\n",
            "\u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\BIAI.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, trainer, train_data, train_logger, epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000011?line=3'>4</a>\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000011?line=4'>5</a>\u001b[0m train_logger\u001b[39m.\u001b[39mstart(epoch)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000011?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(net, train_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000011?line=6'>7</a>\u001b[0m train_logger\u001b[39m.\u001b[39mlog_epoch(loss)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sotor/repos/veggie/BIAI.ipynb#ch0000011?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m* Finished training epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\network.py:114\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, network, loader)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/network.py?line=110'>111</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/network.py?line=112'>113</a>\u001b[0m network\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m--> <a href='file:///c%3A/Users/sotor/repos/veggie/network.py?line=113'>114</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/network.py?line=114'>115</a>\u001b[0m     x, expected \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/network.py?line=116'>117</a>\u001b[0m     network\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:232\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/datasets/folder.py?line=229'>230</a>\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/datasets/folder.py?line=230'>231</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/datasets/folder.py?line=231'>232</a>\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/datasets/folder.py?line=232'>233</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/datasets/folder.py?line=233'>234</a>\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:952\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=943'>944</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=944'>945</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=945'>946</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=946'>947</a>\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=949'>950</a>\u001b[0m \u001b[39m        PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=950'>951</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=951'>952</a>\u001b[0m     i, j, h, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mratio)\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=952'>953</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mresized_crop(img, i, j, h, w, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpolation)\n",
            "File \u001b[1;32mc:\\Users\\sotor\\repos\\veggie\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:919\u001b[0m, in \u001b[0;36mRandomResizedCrop.get_params\u001b[1;34m(img, scale, ratio)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=916'>917</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=917'>918</a>\u001b[0m     target_area \u001b[39m=\u001b[39m area \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mempty(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39muniform_(scale[\u001b[39m0\u001b[39m], scale[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mitem()\n\u001b[1;32m--> <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=918'>919</a>\u001b[0m     aspect_ratio \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mexp(torch\u001b[39m.\u001b[39;49mempty(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49muniform_(log_ratio[\u001b[39m0\u001b[39;49m], log_ratio[\u001b[39m1\u001b[39;49m]))\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=920'>921</a>\u001b[0m     w \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(math\u001b[39m.\u001b[39msqrt(target_area \u001b[39m*\u001b[39m aspect_ratio)))\n\u001b[0;32m    <a href='file:///c%3A/Users/sotor/repos/veggie/venv/lib/site-packages/torchvision/transforms/transforms.py?line=921'>922</a>\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(math\u001b[39m.\u001b[39msqrt(target_area \u001b[39m/\u001b[39m aspect_ratio)))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(net, trainer, train_data, train_logger, 1)\n",
        "net.save(MODEL_OUTPUT_PATH)\n",
        "test(net, tester, test_data, test_logger)\n",
        "\n",
        "train_logger.append = True\n",
        "test_logger.append = True\n",
        "for epoch in range(1, EPOCHS):\n",
        "  train(net, trainer, train_data, train_logger, epoch + 1)\n",
        "  net.save(MODEL_OUTPUT_PATH)\n",
        "  test(net, tester, test_data, test_logger)\n",
        "\n",
        "net.save(MODEL_OUTPUT_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BIAI",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
